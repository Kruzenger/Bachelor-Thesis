\section{Анализ технологий и требований}
\label{sec:Chapter2} \index{Chapter2}
    Идея потоковой обработки больших объёмов данный появилась десяток лет назад и активно развивается с каждым годом. Это мотивирует появление новых технологий, компонентов и архитектур. В этой главе мы рассмотрим самый распространённые и актуальные для исследования технологии.   

    \subsection{Существующие архитектуры}
        С появлением задач потоковой обработки данных стали развиваться архитектуры, специализированные на решении данных задач. Далее рассматриваются самые популярные из них: Lambda-архитектура, Каппа-архитектура и Microservices + Event Sourcing:
        \subsubsection{Lambda-архитектура}
            Lambda-архитектура представляет собой гибридную модель проектирования системы обработки данных, направленную на преодоление фундаментального противоречия между низкой задержкой и высокой точностью. Данная парадигма достигает баланса за счёт декомпозиции вычислительного процесса на два независимых, но синхронизированных слоя: пакетный (batch) и скоростной (speed).
            
            Ядро Lambda-архитектуры базируется на трёх инвариантах:
            \begin{enumerate}
                \item \textbf{Неизменяемость данных (Immutability)} — Все входящие события сохраняются в сыром виде в устойчивом хранилище без модификаций.
                \item \textbf{Принцип пересчёта (Recomputation)} — Любое производное представление генерируется исключительно на основе первичных данных через детерминированные функции.
                \item \textbf{Иерархия точности} — Пакетный слой формирует эталонные результаты на полном историческом срезе, тогда как скоростной слой компенсирует его инерционность приближёнными инкрементальными вычислениями.
            \end{enumerate}
            
            Далее рассмотрим архитектурные компоненты и их взаимодействие:
            \begin{itemize}
                \item \textbf{Пакетный слой (Batch Layer)} функционирует как источник истины (source of truth). Он обрабатывает полный архив неизменяемых данных, используя распределённые вычислительные модели типа MapReduce. Результатом являются предварительно агрегированные представления (batch views), например, суточные статистики пользовательской активности. Эти представления материализуются в высокопроизводительных хранилищах типа колоночных баз данных. Ключевой характеристикой слоя является гарантированная точность, достигаемая за счёт полного пересчёта при изменении алгоритмов.
                \item \textbf{Скоростной слой (Speed Layer)} оперирует исключительно новыми событиями, ещё не учтёнными в batch views. Его задача — минимизировать задержку предоставления актуального состояния. Для этого применяются потоковые движки (Apache Storm, Flink), обрабатывающие данные в режиме реального времени. Результаты фиксируются в отдельном хранилище (real-time views), например, key-value базах. Слой жертвует точностью в пользу скорости: возможны временные расхождения из-за отсутствия исторического контекста.
                \item \textbf{Сервисный слой (Serving Layer)} обеспечивает согласованность данных для потребителей. При выполнении запроса он объединяет результаты batch views и real-time views через операцию слияния. Например, суммарное количество транзакций за текущий день будет вычислено как: 
                \begin{equation}    
                    [result] = [speed\_statistics] + [batch\_statistics]
                \end{equation}
            \end{itemize}
            
            
            Несмотря на концептуальную стройность, Lambda-архитектура сталкивается с существенными проблемами в эксплуатации:
            \begin{itemize}
                \item \textbf{Семантический разрыв (Semantic Gap)} — Дупликация бизнес-логики в batch- и speed-слоях ведёт к рискам несогласованности. Например, оконная агрегация в Spark (batch) и Flink (stream) может давать различные результаты из-за различий в реализации.
                \item \textbf{Эксплуатационная сложность} — Поддержка двух независимых вычислительных кластеров (Hadoop для batch, Storm/Flink для stream) требует значительных ресурсов и координации. Синхронизация форматов данных, версий кода и метаданных становится нетривиальной задачей.
                \item \textbf{Латентность обновления эталонных данных} — Периодичность пересчёта batch views (например, раз в сутки) создаёт окна неактуальности, критичные для приложений реального времени.
                \item \textbf{Избыточность хранения} — Сырые данные дублируются в хранилищах для batch (HDFS) и stream (Kafka), увеличивая затраты.
            \end{itemize}
            
            
            Отраслевая практика показала, что классическая Lambda-архитектура эффективна лишь для узкого класса задач (например, финансовый аудит). В ответ на её недостатки возникли модификации:
            \begin{itemize}
                \item \textbf{Гибридные вычисления (Unified Processing)} — Использование фреймворков типа Apache Beam позволяет выполнять один код в режиме batch и stream, устраняя дублирование логики.
                \item \textbf{Оптимизация через Kappa-принципы} — Исторические данные воспроизводятся из потокового лога (Kafka с долгим хранением), сокращая зависимость от пакетного слоя.
                \item \textbf{Stateful Stream Processing} — Движки вроде Flink минимизируют разрыв между точностью и скоростью за счёт управляемых состояний (managed state) и чекипоинтов.
            \end{itemize}
            
            
            В заключении можно сказать, что Lambda-архитектура остаётся значимой теоретической моделью, иллюстрирующей онтологический конфликт между требованиями к задержке и точности в распределённых системах. Её главное наследие — явное разделение ответственности за консистентность (batch) и оперативность (speed). Однако операционные издержки и развитие потоковых технологий смещают фокус в сторону унифицированных подходов. Современные реализации заимствуют идею иерархии точности, но реализуют её через детерминированную реплей данных и транзакционные обновления состояний, что снижает необходимость в дуализме вычислений. Для новых систем целесообразна оценка компромиссов между чистотой Lambda и прагматикой Каппа-архитектур с учётом специфики предметной области.

        \subsection{Kappa Архитектура}
            Каппа-архитектура возникла как методологический ответ на операционные сложности Lambda-подхода, предлагая принципиально иную философию проектирования. Её ядро составляет концепция единого вычислительного конвейера, где все данные — исторические и реального времени — обрабатываются через единую потоковую модель. Эта парадигма устраняет дуализм вычислений, характерный для Lambda, за счёт абстракции неизменяемого лога событий как универсального источника истины.

            
            Каппа-архитектура базируется на трёх фундаментальных принципах:
            \begin{enumerate}
                \item \textbf{Централизация на потоковом логе} — Все входящие события бессрочно сохраняются в распределённом брокере сообщений (Apache Kafka, Pulsar) с гарантией сохранности и порядковой целостности.
                \item \textbf{Обработка через реплей} — Для пересчёта результатов при изменении логики или восстановлении после сбоев система заново обрабатывает релевантные сегменты лога.
                \item \textbf{Детерминизм вычислений} — Повторный прогон тех же данных через обновлённую логику обязан давать консистентный результат, что обеспечивается неизменяемостью событий и идемпотентностью операторов.
            \end{enumerate}

            
            Рассматривая Каппа-Архитектуру более подробно можно выделить следующие \\структурные компоненты:
            \begin{enumerate}
                \item Неизменяемое хранилище событий выполняет роль первичного источника данных. Брокер сообщений организует события в упорядоченные, секционированные последовательности (топики), где каждая запись снабжается монотонно растущим смещением (offset). Долговременное хранение (ретеншн) обеспечивает доступ к полному историческому архиву — от часов до лет в зависимости от требований.
                \item Единый вычислительный слой заменяет дублирующие batch- и speed-компоненты Lambda. Потоковый движок (Apache Flink, Spark Structured Streaming) потребляет данные непосредственно из лога, применяя пользовательскую логику. Ключевая инновация — поддержка репроцессинга: при необходимости система может перезапустить обработку с произвольной точки смещения, игнорируя текущее состояние. Для управления таким пересчётом используются:
                \begin{itemize}
                    \item \textbf{Точки восстановления (savepoints)} — снапшоты состояния вычислений;
                    \item \textbf{Водяные знаки (watermarks)} — механизмы контроля полноты данных при обработке по времени событий.
                \end{itemize}
                \item Сервисный интерфейс предоставляет результаты через материализованные представления. В отличие от Lambda, здесь отсутствует операция слияния — актуальное состояние генерируется исключительно потоковым конвейером и сохраняется в OLAP-хранилищах (ClickHouse), key-value базах (Redis) или специализированных движках (Apache Pinot).
            \end{enumerate}

            
            Говоря про математическую модель архитектуру стоит выделить процесс обработки данных. Выглядит он следующим образом:
            Результат вычислений $R$ в момент $t$ определяется как функция от полного набора событий $S_{[0,t]}$:
            \begin{equation}
                R(t) = \Phi(S_{[0,t]}, \Omega)
            \end{equation}

            где:
            \begin{itemize}
                \item $\Phi$ — потоковая функция обработки,
                \item $\Omega$ — конфигурация состояния (окна, агрегаторы, UDF).
            \end{itemize}
            При изменении $\Omega$ (например, исправлении алгоритма) результат пересчитывается как 
            \begin{equation}
                R`(t)=\Phi_{new}(S_{[0,t]},\Omega_{new}).  
            \end{equation}

            Несмотря на элегантность подхода, архитектура предъявляет жёсткие требования к инфраструктуре:
            \begin{itemize}
                \item \textbf{Производительность брокера} — Длительный ретеншн требует экстремальной масштабируемости хранилища. Например, топик Kafka с ретеншном 1 год при нагрузке 1 TB/день потребует ~365 TB дискового пространства с учётом репликации.
                \item \textbf{Детерминизм операций} — Некорректная обработка времени событий (временных штампов) или неидемпотентные операции приведут к расхождениям при реплее.
                \item \textbf{Ресурсоёмкость репроцессинга} — Полный пересчёт истории может занимать часы, блокируя ресурсы кластера. Техники инкрементального обновления состояний (например, в Flink) смягчают, но не решают проблему.
                \item \textbf{Сложность отладки} — Анализ причин расхождений между разными версиями $R()$ и $R`()$ требует трассировки событий по всему конвейеру.
            \end{itemize}

            
            В сравнении с Lambda-архитектурой, Каппа-архитектура устраняет три фундаментальных ограничения:
            \begin{enumerate}
                \item \textbf{Ликвидация дублирования логики} — Единый код для всех данных вместо раздельных реализаций под batch/stream.
                \item \textbf{Упрощение инфраструктуры} — Отказ от пакетного кластера (Hadoop/Spark) в пользу единого потокового стека.
                \item \textbf{Гарантия согласованности} — Отсутствие семантического разрыва между "точными" и "приближёнными" представлениями.
            \end{enumerate}

            
            Однако Каппа менее устойчива к ошибкам в проектировании: некорректная водяная метка или потеря состояния при реплее фатальны. Lambda же изолирует риски в speed-слое, сохраняя эталонные данные в batch.

            
            Итого, Каппа-архитектура представляет собой эволюционный шаг в направлении унификации вычислительных моделей. Её главное достижение — доказательство возможности построения консистентных систем реального времени без дуализма вычислений. Хотя операционные требования остаются высокими, развитие управляемых сервисов (Confluent Cloud, Amazon MSK) и стандартизация семантик "ровно один раз" (Kafka Transactions, Flink Checkpoints) снижают барьер внедрения. Для современных разработок Каппа служит предпочтительной основой при условии: долговременной сохраняемости событий в брокере, детерминированности бизнес-логики и готовности к компромиссам в ресурсоёмкости репроцессинга. В контексте научной дискуссии она демонстрирует сдвиг парадигмы: от компромиссов между точностью и скоростью — к их синтезу через иммутабельные данные и реиграцию состояний.

        \subsubsection{Microservices + Event Sourcing}
            Микросервисная архитектура, интегрированная с принципами проектирования, опирающихся на события (Event Sourcing), формирует парадигму, где потоки событий становятся центральной осью координации и сохранения состояния. Данный подход трансформирует традиционные модели взаимодействия сервисов через API в декомпозированную систему асинхронной коммуникации, основанной на неизменяемых фактах.
            
            
            Философия архитектуры базируется на двух взаимодополняющих принципах:
            \begin{enumerate}
                \item \textbf{Проектирование с упором на события (Event Sourcing)} — Состояние системы выводится исключительно из последовательности событий, фиксирующих изменения. В отличие от CRUD-моделей, где актуальное состояние хранится напрямую, здесь реконструкция объекта осуществляется путём применения событий в порядке их возникновения.
                \item \textbf{Декомпозиция на микросервисы} — Каждый сервис инкапсулирует собственную бизнес-логику и данные, взаимодействуя с другими исключительно через события. Это устраняет жесткие зависимости, характерные для монолитных систем.
            \end{enumerate}

            
            Ключевым объектом подобного подхода являются брокеры сообщений (Apache Kaf-ka, RabbitMQ), которые одновременно выступают потоками, связывающими сервисы и журналом команд, гарантирующим сохранность и порядок событий.\\

            \textbf{Механизмы взаимодействия компонентов:}
            
            Генерация событий инициируется командами, поступающими в сервисы через общий шлюз, часто называемый Gateway, или от других сервисов. Например, сервис обработки заказов при подтверждении платежа генерирует событие OrderPaid, которое публикуется в соответствующий топик. Каждое событие обычно содержит:
            \begin{itemize}
                \item Уникальный идентификатор агрегата (например, order\_id)
                \item Таймстамп генерации
                \item Полезную нагрузку в формате, независимом от потребителей
            \end{itemize}

            
            Потребители событий (микросервисы-консьюмеры) подписываются на релевантные топики, обновляя свои локальные состояния или инициируя новые процессы. Сервис аналитики может использовать OrderPaid для расчёта метрик выручки, а сервис доставки — для планирования маршрутов.
            
            
            Однако данный подход имеет ряд вызовов и проблем, которые усложняют работу:
            \begin{itemize}
                \item \textbf{Сложность отслеживания причинности} — В распределённых системах события могут поступать с задержками, нарушая локальный порядок. Механизмы водяных знаков (watermarks) и векторных часов частично решают проблему.
                \item \textbf{Управление схемами данных} — Эволюция форматов событий требует стратегий обратной совместимости (Schema Registry, Avro).
                \item \textbf{Ресурсоёмкость пересчёта состояния} — Длительные цепочки событий замедляют восстановление сервисов после сбоев. Оптимизация через снапшоты (snap-shots) текущих состояний.
                \item \textbf{Отладка распределённых процессов} — Трассировка транзакций, затрагивающих несколько сервисов, требует инструментов вроде OpenTelemetry.
                \item \textbf{Гарантии доставки} — Хотя брокеры типа Kafka обеспечивают "ровно один раз" семантику, идемпотентность обработчиков остаётся ответственностью разработчика.
            \end{itemize}

            
            Паттерн CQRS (Command Query Responsibility Segregation) дополняет модель и решает часть проблем, разделяя операции записи и чтения. Синхронизация между частями в таком подходе обеспечивается асинхронно через поток событий, что устраняет блокировки при высокой нагрузке. В таком подходе модель разделяется на следующие компоненты:
            \begin{enumerate}
                \item \textbf{Командная сторона (Write Model)} — обрабатывает команды, валидирует их и генерирует события.
                \item \textbf{Сторона запросов (Read Model)} — строит материализованные представления из потока событий (например, в Elasticsearch для полнотекстового поиска).
            \end{enumerate}

            
            В сравнении с другими архитектурами, микросервисных подход в связке с проектированием, опирающимся на события, имеет следующие приемущества:
            \begin{itemize}
                \item \textbf{Аудит и воспроизводимость} — Полная история событий позволяет реконструировать состояние системы на любой момент времени.
                \item \textbf{Отказоустойчивость} — Потеря состояния сервиса компенсируется переигранием событий из журнала.
                \item \textbf{Гибкая эволюция} — Добавление новых сервисов не требует изменения существующих: достаточно подписки на релевантные события.
                \item \textbf{Масштабируемость} — Независимое шардирование топиков и консьюмерских групп распределяет нагрузку.
                \item \textbf{Согласованность} — Асинхронная модель гарантирует консистентность без распределённых транзакций.
            \end{itemize}

    \subsection{Системы обработки событий}
        Одними из ключевых технологий в задаче потоковой обработки данных являются системы обработки событий. Они выступают в качестве главных артерий системы и решают задачу общения между различными компонентами систимы, а так же занимаются вопросом перемещения больших объёмов данных. Такие системы чаще всего работают с понятиями "потока" или "очереди", которые является основной сущностью, используемой для передачи данных. Вот самые распространённые из них:
        
        \subsubsection{Apache Kafka}
            Apache Kafka представляет собой распределённую платформу для потоковой передачи и обработки данных, разработанную для обеспечения высокой пропускной способности, минимальных задержек и устойчивости к сбоям. Её архитектура основана на модели публикации-подписки, что позволяет эффективно управлять непрерывными потоками информации между разнородными источниками и приложениями. Ключевой особенностью системы является способность обрабатывать миллионы событий в секунду, сохраняя при этом целостность и порядок данных даже в условиях масштабирования или частичных отказов узлов.
            
            
            Основу Kafka составляют топики — логические каналы, через которые данные распределяются между участниками системы. Каждый топик делится на партиции, обеспечивающие параллелизм обработки: записи в рамках партиции упорядочены, что гарантирует консистентность, а распределение партиций между узлами кластера позволяет балансировать нагрузку. Продюсеры — компоненты, ответственные за публикацию данных в топики — взаимодействуют с брокерами (серверами Kafka), которые сохраняют записи на диск с заданной политикой хранения. Консьюмеры, в свою очередь, извлекают данные из топиков, поддерживая как индивидуальное, так и групповое потребление, что обеспечивает гибкость в распределении задач.
            
             
            Важным аспектом архитектуры Kafka является её отказоустойчивость. Механизм репликации позволяет создавать копии партиций на нескольких брокерах, гарантируя доступность данных при выходе узлов из строя. Для координации метаданных кластера (например, информации о расположении партиций) ранее использовался Apache ZooKeeper, однако современные версии Kafka постепенно внедряют встроенные решения для уменьшения внешних зависимостей.
            
            
            Система поддерживает несколько семантик доставки сообщений:
            \begin{itemize}
                \item \textbf{"максимум один раз"} — данные могут быть потеряны, но не доставлены повторно;
                \item \textbf{"минимум один раз"} — гарантируется минимум одна доставка, что может приводить к дублированию;
                \item \textbf{"ровно один раз"} — исключает как потери, так и повторения, достигая идемпотентности операций.
            \end{itemize}

        \subsubsection{Apache Flink}
            Apache Flink представляет собой высокопроизводительный фреймворк с открытым исходным кодом, ориентированный на обработку потоковых и пакетных данных в гибридном формате. В отличие от систем, разделяющих подходы к работе с потоками и статическими наборами данных, Flink унифицирует эти модели через абстракцию бесконечных потоков, где пакетная обработка рассматривается как частный случай потоковой с фиксированным временным окном. Это позволяет разработчикам создавать гибридные приложения, сочетающие аналитику в реальном времени и исторические вычисления.
            
            
            Ключевой особенностью архитектуры Flink является управление состоянием (state management). Система сохраняет промежуточные результаты обработки событий, обеспечивая консистентность даже при сбоях узлов. Механизм чекпоинтов (checkpoints) периодически фиксирует состояние всех операторов в распределённом хранилище, что гарантирует восстановление данных с точностью до последнего корректного состояния. Для достижения семантики "ровно один раз" Flink использует двухфазный протокол фиксации транзакций, что исключает дублирование или потерю информации — критически важное свойство для финансовых систем или аудита.
            
            
            Одним из преимуществ Flink перед аналогами (например, Apache Spark) является нативная поддержка потоковой обработки. Вместо имитации потоков через микропакеты Flink оперирует непрерывными данными, что снижает задержки и повышает эффективность ресурсов. Архитектура фреймворка включает TaskManager (исполнительные узлы) и JobManager (оркестратор задач), распределяющий операции по кластеру. Для балансировки нагрузки Flink динамически перераспределяет задачи между узлами, используя модель data parallelism.
            
            
            Важным аспектом Flink является работа с временными метками. Система различает:
            \begin{itemize}
                \item \textbf{Event time (временной штамп)} — время генерации события источником
                \item \textbf{Processing time} — момент обработки события фреймворком
                \item \textbf{Ingestion time} — время поступления данных в Flink. Поддержка временных штампов позволяет корректно обрабатывать задержанные или неупорядоченные данные, что актуально для IoT-устройств или глобальных систем с неравномерной задержкой сети.
            \end{itemize}
            
            
            Flink предоставляет богатый набор операторов для трансформации данных:
            \begin{itemize}
                \item \textbf{Оконные функции (tumbling, sliding, session windows)} — группировка событий по временным или количественным критериям;
                \item \textbf{CEP (Complex Event Processing)} — обнаружение паттернов в потоках (например, последовательность действий пользователя);
                \item \textbf{Интеграция с ML-библиотеками} — выполнение прогнозных моделей в реальном времени.
            \end{itemize}

        \subsubsection{RabbitMQ}
            RabbitMQ представляет собой брокер сообщений с открытым исходным кодом, реализующий протокол AMQP (Advanced Message Queuing Protocol). Его основная задача — обеспечить надёжную асинхронную коммуникацию между компонентами распределённых систем, что особенно актуально в микросервисных архитектурах и сценариях, требующих декомпозиции задач. В отличие от систем, ориентированных на потоковую обработку, RabbitMQ фокусируется на гарантированной доставке сообщений, управлении очередями и балансировке нагрузки между потребителями.
            
            
            Архитектурные компоненты системы включают:
            \begin{itemize}
                \item \textbf{Обменники (Exchanges)} — точки входа для сообщений, определяющие правила маршрутизации.
                \item \textbf{Очереди (Queues)} — буферы для хранения сообщений до их обработки потребителями.
                \item \textbf{Привязки (Bindings)} — правила, связывающие обменники с очередями на основе ключей маршрутизации.
            \end{itemize}
            
            
            Сообщения публикуются продюсерами в обменники, которые, используя заданный тип (direct, topic, fanout, headers), определяют, в какие очереди они будут направлены. Например, обменник типа fanout рассылает сообщения во все привязанные очереди, а topic позволяет использовать шаблоны для гибкой фильтрации.
            
            
            Гарантии доставки обеспечиваются механизмами подтверждений (acknowledgments). Потребитель отправляет брокеру подтверждение после успешной обработки сообщения. Если подтверждение не получено (например, из-за сбоя), RabbitMQ повторно ставит сообщение в очередь. Для критически важных задач поддерживается режим persistent messages, при котором данные сохраняются на диск, предотвращая потерю при перезапуске сервера.
            
            
            Кластеризация повышает отказоустойчивость системы. RabbitMQ позволяет объединять узлы в кластеры с общими метаданными (информацией об обменниках, очередях), но данные очередей хранятся локально на каждом узле. Для обеспечения избыточности используется зеркалирование очередей (mirrored queues), при котором сообщения реплицируются между узлами. Это минимизирует риск потери данных при выходе из строя отдельных серверов.
            
            Примеры применения RabbitMQ охватывают широкий спектр задач:
            \begin{itemize}
                \item \textbf{Асинхронная обработка задач} — вынесение ресурсоёмких операций (генерация отчётов, обработка изображений) в фоновый режим.
                \item \textbf{Балансировка нагрузки} — распределение запросов между несколькими экземплярами сервиса через конкурентных потребителей.
                \item \textbf{Интеграция разнородных систем} — обеспечение взаимодействия между legacy-приложениями и современными микросервисами через унифицированный интерфейс обмена сообщениями.
            \end{itemize}
            
            В отличие от систем, оптимизированных для потоковой аналитики (например, Apa-che Kafka), RabbitMQ не предназначен для обработки экстремально высоких объёмов данных в реальном времени. Однако его сила заключается в точном управлении жизненным циклом сообщений, гибкой маршрутизации и поддержке сложных сценариев взаимодействия, таких как RPC или распределённые транзакции.

        \subsubsection{Apache Spark Streaming}
            Apache Spark Streaming представляет собой модуль фреймворка Apache Spark, предназначенный для обработки потоков данных через микропакетную модель (micro-bat-ching). В отличие от систем, ориентированных на непрерывную потоковую обработку (например, Flink), Spark Streaming делит входящий поток на небольшие пакеты фиксированного интервала (от миллисекунд до секунд), обрабатывая их как статические RDD-наборы. Это позволяет использовать единую кодовую базу и API для пакетной и потоковой аналитики, обеспечивая согласованность в гибридных сценариях.
            
            
            Он состоит из следующих архитектурных компонент:
            \begin{itemize}
                \item \textbf{Драйвер} — Оркестрирует задачи, разбивает поток на микропакеты (DStreams) и распределяет их по исполнителям.
                \item \textbf{Исполнители} — Обрабатывают пакеты данных на рабочих узлах кластера.
                \item \textbf{Высокоуровневый API} — Предоставляет абстракцию "бесконечной таблицы"\\ над потоком, позволяя применять SQL-подобные операции и декларативные запросы.
            \end{itemize}
            
            
            Ключевыми особенностями можно выделить:
            \begin{itemize}
                \item \textbf{Гарантии "ровно один раз"} — Достигается через комбинацию чекпоинтов состояния (через HDFS/S3) и атомарной записи результатов (начиная с Structured Streaming).
                \item \textbf{Интеграция с экосистемой Spark} — Поддержка MLlib (машинное обучение в реальном времени), GraphX (графовая обработка) и Spark SQL.
                \item \textbf{Обработка по времени событий} — Возможность агрегации данных по временным окнам с учетом меток генерации события, а не времени поступления.
                \item \textbf{Восстановление через чекпоинты} — Восстановление состояния приложения после сбоя из контрольных точек.
            \end{itemize}
            
            
            В отличие от Apache Flink, использующего нативную потоковую модель, Spark Streaming жертвует сверхнизкой задержкой (типично 0.5–2 секунды) в пользу интеграции с пакетной обработкой Spark и богатой экосистемой. Для сценариев, требующих строгого порядка событий и латенции в миллисекунды, предпочтительнее Flink; для аналитических задач с умеренной задержкой и унифицированной кодобазой — Spark Streaming.

        \subsubsection{Apache Storm}
            Apache Storm представляет собой распределённый, отказоустойчивый движок реального времени (true streaming) для обработки неограниченных потоков данных с экстремально низкой задержкой (миллисекунды). Разработанный в Twitter для анализа твитов в реальном времени, Storm фокусируется на простой, высокопроизводительной доставке и трансформации событий, где порядок обработки каждого сообщения критичен. Его архитектура основана на направленном ациклическом графе (DAG) задач, называемом топологией (topology), которая определяет поток данных и логику обработки.
            
            
            Архитектурные компоненты:
            \begin{itemize}
                \item \textbf{Nimbus} — Центральный координатор (аналог JobManager в Flink), отвечающий за распределение кода топологии по рабочим узлам, планирование задач и мониторинг сбоев. Является "главным" узлом кластера (но без единой точки отказа при использовании HA-режима).
                \item \textbf{Supervisor} — Демон, работающий на каждом рабочем узле кластера. Получает задания от Nimbus, запускает и останавливает рабочие процессы (Worker processes), выделенные под выполнение конкретных задач топологии.
                \item \textbf{Worker process} — JVM-процесс, выполняющий часть топологии. Запускает внутри себя Executor'ы (потоки), которые выполняют Task'и (экземпляры компонентов spout или bolt).
                \item \textbf{Spout} — Источник данных в топологии. Читает данные из внешних систем (Kafka, Kinesis, очереди, сокеты) и испускает кортежи (tuples) в топологию. Отвечает за надёжность: отслеживает обработку испущенных кортежей и повторяет отправку при сбоях (в режимах с гарантиями).
                \item \textbf{Bolt} — Обработчик данных. Принимает кортежи от spout'ов или других bolt'ов, выполняет над ними операции (фильтрация, агрегация, обогащение, запись в БД), и может испускать новые кортежи дальше по топологии. Bolt'ы образуют вычислительные вершины графа.
            \end{itemize}
            
                
            Одной из ключевых особенностей Apache Storm является модель True Streaming обработка событий по одному сразу при их поступлении, в отличие от микропакетных моделей (Spark Streaming). Это обеспечивает минимально возможную задержку. Так же отдельно стоит упомянуть возможноть для горизонтального масштабирования системы: Она легко масштабируется добавлением новых рабочих узлов. Параллелизм spout/bolt настраивается независимо.
            
            
            Apache Storm так же может похвастаться отказоустойчивостью: При падении рабочего узла (Worker) Nimbus перезапускает его задачи на других узлах. При падении самого Nimbus, работающие Supervisor'ы и Worker'ы продолжают функционировать до его восстановления (режим HA требует резервного Nimbus и ZooKeeper).
            
            
            Apache Storm так же поддерживает те же гарантии доставки сообщений, что и Apache Kafka:
            \begin{itemize}
                \item \textbf{"максимум один раз" (По умолчанию)} — Кортежи могут быть потеряны при сбое, но не обработаны повторно (производительность выше).
                \item \textbf{"минимум один раз"} — Гарантирует, что каждый кортеж будет обработан минимум один раз (возможны дубли). Достигается механизмом "acknowledgment" (ACK): spout отслеживает кортежи и переиспускает не подтверждённые в течение таймаута.
                \item \textbf{"ровно один раз" (через Trident)} — Предоставляется высокоуровневой абстракцией Trident. Trident группирует кортежи в мелкие пакеты (микробатчи), использует уникальные ID для отслеживания и обеспечивает идемпотентность операций или транзакционное обновление состояния. Однако это увеличивает задержку и сложность.
            \end{itemize}
            
            Для Apache Storm можно выделить следующие отличия от аналогов:
            \begin{enumerate}
                \item \textbf{Apache Kafka} — Storm является кластерным фреймворком (требует Nimbus/Su-pervisors или YARN/K8s), тогда как Kafka — это легковесная система, интегрируемая почти в любое приложение и используемая для координации и хранения состояния. Kafka Streams проще для Kafka-центричных пайплайнов, но Storm универсальнее для источников, не связанных с Kafka.
                \item \textbf{Apache Flink} — Storm имеет более слабую поддержку управления состоянием (state management) и обработки по времени события (временных штампов) по сравнению с Flink. Trident добавляет состояние и "ровно один раз" семантику, но за счет задержки и усложнения модели программирования. Flink изначально создан для управления состоянием и работы с штампами времени событий. Storm имеет меньшую абстрактную задержку, но Flink часто превосходит его в пропускной способности на сложных состояниях.
                \item \textbf{Apache Spark Streaming} — Storm обеспечивает на порядок меньшую задержку (миллисекунды против секунд), так как обрабатывает события по одному, а не микропакетами. Однако Spark Streaming проще интегрируется с пакетной обработкой Spark и экосистемой (MLlib, SQL) и предоставляет более сильные гарантии состояния и временных штампов "из коробки" через Structured Streaming.
            \end{enumerate}

            
            Storm остаётся эффективным выбором для задач, требующих минимально возможной задержки реакции на событие (например, мониторинг сетевой безопасности, простые фильтрации/обогащения в реальном времени, триггеры мгновенных реакций) и где сложное состояние или обработка по временным штампам не являются основным требованием. Его относительная простота также может быть преимуществом для конкретных сценариев. Однако для сложной аналитики с окнами, точным временем событий и богатым состоянием Flink или Spark Structured Streaming часто предпочтительнее.
\newpage
